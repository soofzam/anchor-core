# anchor-core
An open, vendor-neutral standard for epistemically responsible AI answers.
Anchor Core defines minimum behavioral requirements for responsible large language model (LLM) outputs.

It focuses on two systemic failure classes:

Undisclosed hallucination — presenting guesses, fabricated details, or unverified claims as facts.

Context collapse — loss of logical invariants, constraints, or assumptions over long interactions.

These failures are amplified by a structural incentive misalignment in modern LLMs:

LLMs are optimized to produce the most probable and satisfying continuation, not the most logically correct or epistemically responsible one.

Anchor Core does not attempt to replace or retrain models.

Instead, it defines:

A failure taxonomy

A behavioral benchmark (Test Set v0.1)

A compliance specification (Bronze level)

A research-backed foundation for minimum responsible AI behavior

Current Status

Version: v0.1 (Foundational)

This repository currently includes:

Anchor Research Note v0.1

Anchor Test Set v0.1

Anchor Failure Taxonomy v0.1

Anchor Compliance Specification v0.1 (Bronze)

This is an early standard proposal and is intentionally open to critique, replication, and extension.

What Anchor Is Not

Anchor is not:

A new LLM

A prompt trick

A model ranking project

A commercial wrapper

A replacement for retrieval or training

Anchor is a standards initiative focused on epistemic integrity.

Contribution

We welcome:

Replication attempts

Additional stress prompts

Alternative scoring proposals

Failure case documentation

Formal critiques

Standards improve through adversarial discussion.

If Anchor ever has to choose between standards and revenue, it will choose standards.
